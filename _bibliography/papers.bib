@article{cheng2025can,
  title={Can LLMs Reason Abstractly Over Math Word Problems Without CoT? Disentangling Abstract Formulation From Arithmetic
Computation},
  author={Ziling Cheng, Meng Cao, Leila Pishdad, Yanshuai Cao, Jackie CK Cheung},
  journal={https://arxiv.org/abs/2505.23701},
  year={2025},
  abbr={ArXiv},
}

@article{cao2025scar,
  title={SCAR: Shapley Credit Assignment for More Efficient RLHF},
  author={Cao, Meng and Zhang, Shuyuan and Chang, Xiao-Wen and Precup, Doina},
  journal={arXiv preprint arXiv:2505.20417},
  year={2025},
  abbr={ArXiv},
}

@inproceedings{cheng-etal-2025-stochastic,
    title = "Stochastic Chameleons: Irrelevant Context Hallucinations Reveal Class-Based (Mis)Generalization in {LLM}s",
    author = "Cheng, Ziling  and
      Cao, Meng  and
      Rondeau, Marc-Antoine  and
      Cheung, Jackie CK",
    editor = "Che, Wanxiang  and
      Nabende, Joyce  and
      Shutova, Ekaterina  and
      Pilehvar, Mohammad Taher",
    booktitle = "Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2025",
    address = "Vienna, Austria",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2025.acl-long.1458/",
    doi = "10.18653/v1/2025.acl-long.1458",
    pages = "30187--30214",
    ISBN = "979-8-89176-251-0",
    abstract = "The widespread success of LLMs on NLP benchmarks has been accompanied by concerns that LLMs function primarily as stochastic parrots that reproduce texts similar to what they saw during pre-training, often erroneously. But what is the nature of their errors, and do these errors exhibit any regularities? In this work, we examine irrelevant context hallucinations, in which models integrate misleading contextual cues into their predictions. Through behavioral analysis, we show that these errors result from a structured yet flawed mechanism that we term {\_}class-based (mis)generalization{\_}, in which models combine abstract class cues with features extracted from the query or context to derive answers. Furthermore, mechanistic interpretability experiments on Llama-3, Mistral, and Pythia across 39 factual recall relation types reveal that this behavior is reflected in the model{'}s internal computations: (i) abstract class representations are constructed in lower layers before being refined into specific answers in higher layers, (ii) feature selection is governed by two competing circuits {---} one prioritizing direct query-based reasoning, the other incorporating contextual cues {---} whose relative influences determine the final output. Our findings provide a more nuanced perspective on the stochastic parrot argument: through form-based training, LLMs can exhibit generalization leveraging abstractions, albeit in unreliable ways based on contextual cues {---} what we term {\_}stochastic chameleons{\_}."
}

@inproceedings{hong-etal-2025-reasoning,
    title = "The Reasoning-Memorization Interplay in Language Models Is Mediated by a Single Direction",
    author = "Hong, Yihuai  and
      Cao, Meng  and
      Zhou, Dian  and
      Yu, Lei  and
      Jin, Zhijing",
    editor = "Che, Wanxiang  and
      Nabende, Joyce  and
      Shutova, Ekaterina  and
      Pilehvar, Mohammad Taher",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2025",
    month = jul,
    year = "2025",
    address = "Vienna, Austria",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2025.findings-acl.1111/",
    doi = "10.18653/v1/2025.findings-acl.1111",
    pages = "21565--21585",
    ISBN = "979-8-89176-256-5",
    abstract = "Large language models (LLMs) excel on a variety of reasoning benchmarks, but previous studies suggest they sometimes struggle to generalize to unseen questions, potentially due to over-reliance on memorized training examples. However, the precise conditions under which LLMs switch between reasoning and memorization during text generation remain unclear. In this work, we provide a mechanistic understanding of LLMs' reasoning-memorization dynamics by identifying a set of linear features in the model{'}s residual stream that govern the balance between genuine reasoning and memory recall. These features not only distinguish reasoning tasks from memory-intensive ones but can also be manipulated to causally influence model performance on reasoning tasks. Additionally, we show that intervening in these reasoning features helps the model more accurately activate the most relevant problem-solving capabilities during answer generation. Our findings offer new insights into the underlying mechanisms of reasoning and memory in LLMs and pave the way for the development of more robust and interpretable generative AI systems. Our code and data are at https://github.com/yihuaihong/Linear{\_}Reasoning{\_}Memory{\_}Features."
}

@inproceedings{cao-etal-2024-enhancing,
    title = "Enhancing Reinforcement Learning with Dense Rewards from Language Model Critic",
    author = "Cao, Meng  and
      Shu, Lei  and
      Yu, Lei  and
      Zhu, Yun  and
      Wichers, Nevan  and
      Liu, Yinxiao  and
      Meng, Lei",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.emnlp-main.515/",
    doi = "10.18653/v1/2024.emnlp-main.515",
    pages = "9119--9138",
    abstract = "Reinforcement learning (RL) can align language models with non-differentiable reward signals, such as human preferences. However, a major challenge arises from the sparsity of these reward signals - typically, there is only a single reward for an entire output. This sparsity of rewards can lead to inefficient and unstable learning. To address this challenge, our paper introduces an novel framework that utilizes the critique capability of Large Language Models (LLMs) to produce intermediate-step rewards during RL training. Our method involves coupling a policy model with a critic language model, which is responsible for providing comprehensive feedback of each part of the output. This feedback is then translated into token or span-level rewards that can be used to guide the RL training process. We investigate this approach under two different settings: one where the policy model is smaller and is paired with a more powerful critic model, and another where a single language model fulfills both roles. We assess our approach on three text generation tasks: sentiment control, language model detoxification, and summarization. Experimental results show that incorporating artificial intrinsic rewards significantly improve both sample efficiency and the overall performance of the policy model, supported by both automatic and human evaluation."
}

@inproceedings{yu-etal-2024-mechanistic,
    title = "Mechanistic Understanding and Mitigation of Language Model Non-Factual Hallucinations",
    author = "Yu, Lei  and
      Cao, Meng  and
      Cheung, Jackie CK  and
      Dong, Yue",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2024",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-emnlp.466/",
    doi = "10.18653/v1/2024.findings-emnlp.466",
    pages = "7943--7956",
    abstract = "State-of-the-art language models (LMs) sometimes generate that misalign with world knowledge. To explore the mechanistic causes of these hallucinations, we create diagnostic datasets with subject-relation queries and adapt interpretability methods to trace hallucinations through internal model representations. We discover two general and distinct mechanistic causes of hallucinations shared across LMs (Llama-2, Pythia, GPT-J): 1) : insufficient subject attribute knowledge in lower layer MLPs, and 2) : failure to select the correct object attribute in upper layer attention heads. We also found these two internal mechanistic causes of hallucinations are reflected in external manifestations. Based on insights from our mechanistic analysis, we propose a novel hallucination mitigation method through targeted restoration of the LM{'}s internal fact recall pipeline, demonstrating superior performance compared to baselines."
}

@inproceedings{cao2024successor,
  title={Successor Features for Efficient Multisubject Controlled Text Generation},
  author={Cao, Meng and Fatemi, Mehdi and Cheung, Jackie Chi Kit and Shabanian, Samira},
  booktitle={International Conference on Machine Learning},
  year={2024},
  url={https://arxiv.org/abs/2311.04921},
  abbr={ICML},
}

@inproceedings{
    cao2023systematic,
    title={Systematic Rectification of Language Models via Dead-end Analysis},
    author={Meng Cao and Mehdi Fatemi and Jackie CK Cheung and Samira Shabanian},
    booktitle={International Conference on Learning Representations},
    year={2023},
    url={https://openreview.net/forum?id=k8_yVW3Wqln},
    abbr = {ICLR},
}

@inproceedings{liu-etal-2023-responsible,
    title = "Responsible {AI} Considerations in Text Summarization Research: A Review of Current Practices",
    author = "Liu, Yu Lu  and
      Cao, Meng  and
      Blodgett, Su Lin  and
      Cheung, Jackie Chi Kit  and
      Olteanu, Alexandra  and
      Trischler, Adam",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2023",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-emnlp.413",
    doi = "10.18653/v1/2023.findings-emnlp.413",
    pages = "6246--6261",
    abbr = "EMNLP",
    abstract = "AI and NLP publication venues have increasingly encouraged researchers to reflect on possible ethical considerations, adverse impacts, and other responsible AI issues their work might engender. However, for specific NLP tasks our understanding of how prevalent such issues are, or when and why these issues are likely to arise, remains limited. Focusing on text summarization{---}a common NLP task largely overlooked by the responsible AI community{---}we examine research and reporting practices in the current literature. We conduct a multi-round qualitative analysis of 333 summarization papers from the ACL Anthology published between 2020{--}2022. We focus on how, which, and when responsible AI issues are covered, which relevant stakeholders are considered, and mismatches between stated and realized research goals. We also discuss current evaluation practices and consider how authors discuss the limitations of both prior work and their own work. Overall, we find that relatively few papers engage with possible stakeholders or contexts of use, which limits their consideration of potential downstream adverse impacts or other responsible AI issues. Based on our findings, we make recommendations on concrete practices and research directions.",
}

@inproceedings{cao-etal-2022-learning,
    title = "Learning with Rejection for Abstractive Text Summarization",
    author = "Cao, Meng  and
      Dong, Yue  and
      He, Jingyi  and
      Cheung, Jackie Chi Kit",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.663",
    pages = "9768--9780",
    abbr = {EMNLP},
    abstract = "State-of-the-art abstractive summarization systems frequently hallucinate content that is not supported by the source document, mainly due to noise in the training dataset.Existing methods opt to drop the noisy samples or tokens from the training set entirely, reducing the effective training set size and creating an artificial propensity to copy words from the source. In this work, we propose a training objective for abstractive summarization based on rejection learning, in which the model learns whether or not to reject potentially noisy tokens. We further propose a regularized decoding objective that penalizes non-factual candidate summaries during inference by using the rejection probability learned during training.We show that our method considerably improves the factuality of generated summaries in automatic and human evaluations when compared to five baseline models, and that it does so while increasing the abstractiveness of the generated summaries.",
}

@inproceedings{cao-etal-2022-hallucinated,
    title = "Hallucinated but Factual! Inspecting the Factuality of Hallucinations in Abstractive Summarization",
    author = "Cao, Meng  and
      Dong, Yue  and
      Cheung, Jackie",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.236",
    pages = "3340--3354",
    abbr = {ACL},
    abstract = "State-of-the-art abstractive summarization systems often generate hallucinations; i.e., content that is not directly inferable from the source text. Despite being assumed to be incorrect, we find that much hallucinated content is actually consistent with world knowledge, which we call factual hallucinations. Including these factual hallucinations in a summary can be beneficial because they provide useful background information. In this work, we propose a novel detection approach that separates factual from non-factual hallucinations of entities. Our method is based on an entity{'}s prior and posterior probabilities according to pre-trained and finetuned masked language models, respectively. Empirical results suggest that our method vastly outperforms two baselines in both accuracy and F1 scores and has a strong correlation with human judgments on factuality classification tasks.Furthermore, we use our method as a reward signal to train a summarization system using an off-line reinforcement learning (RL) algorithm that can significantly improve the factuality of generated summaries while maintaining the level of abstractiveness.",
}

@article{cao2022survey,
  title={A Survey on Neural Abstractive Summarization Methods and Factual Consistency of Summarization},
  author={Cao, Meng},
  journal={arXiv preprint arXiv:2204.09519},
  abbr={arXiv},
  year={2022}
}

@inproceedings{cao-etal-2020-factual,
    title = "Factual Error Correction for Abstractive Summarization Models",
    author = "Cao, Meng  and
      Dong, Yue  and
      Wu, Jiapeng  and
      Cheung, Jackie Chi Kit",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.506",
    doi = "10.18653/v1/2020.emnlp-main.506",
    pages = "6251--6258",
    abbr = {EMNLP},
    abstract = "Neural abstractive summarization systems have achieved promising progress, thanks to the availability of large-scale datasets and models pre-trained with self-supervised methods. However, ensuring the factual consistency of the generated summaries for abstractive summarization systems is a challenge. We propose a post-editing corrector module to address this issue by identifying and correcting factual errors in generated summaries. The neural corrector model is pre-trained on artificial examples that are created by applying a series of heuristic transformations on reference summaries. These transformations are inspired by the error analysis of state-of-the-art summarization model outputs. Experimental results show that our model is able to correct factual errors in summaries generated by other neural summarization models and outperforms previous models on factual consistency evaluation on the CNN/DailyMail dataset. We also find that transferring from artificial error correction to downstream settings is still very challenging.",
}

@inproceedings{wu-etal-2020-temp,
    title = "{T}e{MP}: Temporal Message Passing for Temporal Knowledge Graph Completion",
    author = "Wu, Jiapeng  and
      Cao, Meng  and
      Cheung, Jackie Chi Kit  and
      Hamilton, William L.",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.462",
    doi = "10.18653/v1/2020.emnlp-main.462",
    pages = "5730--5746",
    abbr = {EMNLP},
    abstract = "Inferring missing facts in temporal knowledge graphs (TKGs) is a fundamental and challenging task. Previous works have approached this problem by augmenting methods for static knowledge graphs to leverage time-dependent representations. However, these methods do not explicitly leverage multi-hop structural information and temporal facts from recent time steps to enhance their predictions. Additionally, prior work does not explicitly address the temporal sparsity and variability of entity distributions in TKGs. We propose the Temporal Message Passing (TeMP) framework to address these challenges by combining graph neural networks, temporal dynamics models, data imputation and frequency-based gating techniques. Experiments on standard TKG tasks show that our approach provides substantial gains compared to the previous state of the art, achieving a 10.7{\%} average relative improvement in Hits@10 across three standard benchmarks. Our analysis also reveals important sources of variability both within and across TKG datasets, and we introduce several simple but strong baselines that outperform the prior state of the art in certain settings.",
}

@inproceedings{cao-cheung-2019-referring,
    title = "Referring Expression Generation Using Entity Profiles",
    author = "Cao, Meng  and
      Cheung, Jackie Chi Kit",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1312",
    doi = "10.18653/v1/D19-1312",
    pages = "3163--3172",
    abbr = {EMNLP},
    abstract = "Referring Expression Generation (REG) is the task of generating contextually appropriate references to entities. A limitation of existing REG systems is that they rely on entity-specific supervised training, which means that they cannot handle entities not seen during training. In this study, we address this in two ways. First, we propose task setups in which we specifically test a REG system{'}s ability to generalize to entities not seen during training. Second, we propose a profile-based deep neural network model, ProfileREG, which encodes both the local context and an external profile of the entity to generate reference realizations. Our model generates tokens by learning to choose between generating pronouns, generating from a fixed vocabulary, or copying a word from the profile. We evaluate our model on three different splits of the WebNLG dataset, and show that it outperforms competitive baselines in all settings according to automatic and human evaluations.",
}

