---

@article{cao2021inspecting,
  title={Hallucinated but Factual! Inspecting the Factuality of Hallucinations in Abstractive Summarization},
  author={Cao, Meng and Dong, Yue and Cheung, Jackie Chi Kit},
  booktitle="To appear at ACL 2022",
  abbr = {ACL},
}

@inproceedings{cao-etal-2020-factual,
    title = "Factual Error Correction for Abstractive Summarization Models",
    author = "Cao, Meng  and
      Dong, Yue  and
      Wu, Jiapeng  and
      Cheung, Jackie Chi Kit",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.506",
    doi = "10.18653/v1/2020.emnlp-main.506",
    pages = "6251--6258",
    abbr = {EMNLP},
    abstract = "Neural abstractive summarization systems have achieved promising progress, thanks to the availability of large-scale datasets and models pre-trained with self-supervised methods. However, ensuring the factual consistency of the generated summaries for abstractive summarization systems is a challenge. We propose a post-editing corrector module to address this issue by identifying and correcting factual errors in generated summaries. The neural corrector model is pre-trained on artificial examples that are created by applying a series of heuristic transformations on reference summaries. These transformations are inspired by the error analysis of state-of-the-art summarization model outputs. Experimental results show that our model is able to correct factual errors in summaries generated by other neural summarization models and outperforms previous models on factual consistency evaluation on the CNN/DailyMail dataset. We also find that transferring from artificial error correction to downstream settings is still very challenging.",
}

@inproceedings{wu-etal-2020-temp,
    title = "{T}e{MP}: Temporal Message Passing for Temporal Knowledge Graph Completion",
    author = "Wu, Jiapeng  and
      Cao, Meng  and
      Cheung, Jackie Chi Kit  and
      Hamilton, William L.",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.462",
    doi = "10.18653/v1/2020.emnlp-main.462",
    pages = "5730--5746",
    abbr = {EMNLP},
    abstract = "Inferring missing facts in temporal knowledge graphs (TKGs) is a fundamental and challenging task. Previous works have approached this problem by augmenting methods for static knowledge graphs to leverage time-dependent representations. However, these methods do not explicitly leverage multi-hop structural information and temporal facts from recent time steps to enhance their predictions. Additionally, prior work does not explicitly address the temporal sparsity and variability of entity distributions in TKGs. We propose the Temporal Message Passing (TeMP) framework to address these challenges by combining graph neural networks, temporal dynamics models, data imputation and frequency-based gating techniques. Experiments on standard TKG tasks show that our approach provides substantial gains compared to the previous state of the art, achieving a 10.7{\%} average relative improvement in Hits@10 across three standard benchmarks. Our analysis also reveals important sources of variability both within and across TKG datasets, and we introduce several simple but strong baselines that outperform the prior state of the art in certain settings.",
}

@inproceedings{cao-cheung-2019-referring,
    title = "Referring Expression Generation Using Entity Profiles",
    author = "Cao, Meng  and
      Cheung, Jackie Chi Kit",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1312",
    doi = "10.18653/v1/D19-1312",
    pages = "3163--3172",
    abbr = {EMNLP-IJCNLP},
    abstract = "Referring Expression Generation (REG) is the task of generating contextually appropriate references to entities. A limitation of existing REG systems is that they rely on entity-specific supervised training, which means that they cannot handle entities not seen during training. In this study, we address this in two ways. First, we propose task setups in which we specifically test a REG system{'}s ability to generalize to entities not seen during training. Second, we propose a profile-based deep neural network model, ProfileREG, which encodes both the local context and an external profile of the entity to generate reference realizations. Our model generates tokens by learning to choose between generating pronouns, generating from a fixed vocabulary, or copying a word from the profile. We evaluate our model on three different splits of the WebNLG dataset, and show that it outperforms competitive baselines in all settings according to automatic and human evaluations.",
}
